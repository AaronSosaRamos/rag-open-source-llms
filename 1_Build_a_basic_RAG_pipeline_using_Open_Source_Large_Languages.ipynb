{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build RAG pipeline using Open Source Large Languages\n",
        "\n",
        "## Made by: Wilfredo Aaron Sosa Ramos\n",
        "\n",
        "In the notebook we will build a Chat with Website use cases using Zephyr 7B model"
      ],
      "metadata": {
        "id": "J177jX51J1Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "Syag3oIcKHM0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L18fYAOPJzZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323b2fda-e4b3-4e2d-dc13-86b00936b5eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.11)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=d4c72c47277afa2ee2128bc7d88b58aaa5ccb001f14f3fe6c43b309c86e344b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, faiss-cpu, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.28.2\n",
            "    Uninstalling opentelemetry-api-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-api-1.28.2\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.28.2\n",
            "    Uninstalling opentelemetry-sdk-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.28.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 faiss-cpu-1.9.0.post1 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 starlette-0.41.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain faiss-cpu sentence-transformers chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import RAG components required to build pipeline"
      ],
      "metadata": {
        "id": "4FopYAa6Krjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d94OGqtW4vN4",
        "outputId": "76be1660-4b06-45e1-c386-00fad620eee1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.3.24)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.2.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.12 (from langchain_community)\n",
            "  Downloading langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.3.25-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain<0.4.0,>=0.3.12->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_core) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_core) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_core) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_core) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_core) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_core) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_core) (1.2.2)\n",
            "Downloading langchain_community-0.3.12-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.25-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.12-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
            "Downloading langchain_text_splitters-0.3.3-py3-none-any.whl (27 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.3.24\n",
            "    Uninstalling langchain-core-0.3.24:\n",
            "      Successfully uninstalled langchain-core-0.3.24\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.2\n",
            "    Uninstalling langchain-text-splitters-0.3.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.2\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.11\n",
            "    Uninstalling langchain-0.3.11:\n",
            "      Successfully uninstalled langchain-0.3.11\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.12 langchain-text-splitters-0.3.3 langchain_community-0.3.12 langchain_core-0.3.25 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain.vectorstores import FAISS, Chroma\n",
        "from langchain.chains import RetrievalQA, LLMChain"
      ],
      "metadata": {
        "id": "kGXeRCdILsha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90160740-5df9-4124-d43f-59f0b0ad2be9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup HuggingFace Access Token\n",
        "\n",
        "- Log in to [HuggingFace.co](https://huggingface.co/)\n",
        "- Click on your profile icon at the top-right corner, then choose [“Settings.”](https://huggingface.co/settings/)\n",
        "- In the left sidebar, navigate to [“Access Token”](https://huggingface.co/settings/tokens)\n",
        "- Generate a new access token, assigning it the “write” role.\n"
      ],
      "metadata": {
        "id": "DmzD-oWKLtOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "HF_TOKEN = getpass(\"HF Token:\")\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEfA-vDcKrI5",
        "outputId": "fd0a2d44-7970-4e77-b3a0-c5b3ebc2a5c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF Token:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## External data/document - ETL"
      ],
      "metadata": {
        "id": "KFyNhHmsMYBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ziRBTGT3NF7l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEBSITE_URL = \"https://developers.google.com/machine-learning/resources/intro-llms?hl=en\""
      ],
      "metadata": {
        "id": "h7QVgNPmLkKj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(WEBSITE_URL)\n",
        "loader.requests_per_second = 1\n",
        "docs = loader.aload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOrnY8dgMdxt",
        "outputId": "ba818b7e-adba-4835-c4e8-de90110a4191"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  1.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "K1TTrH5QNQxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03e1097-b11a-4e48-f517-16691bd2878c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n          Machine Learning\\n        \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n/\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nDeutsch\\n\\n\\nEspañol\\n\\n\\nEspañol – América Latina\\n\\n\\nFrançais\\n\\n\\nIndonesia\\n\\n\\nItaliano\\n\\n\\nPolski\\n\\n\\nPortuguês – Brasil\\n\\n\\nTiếng Việt\\n\\n\\nTürkçe\\n\\n\\nРусский\\n\\n\\nעברית\\n\\n\\nالعربيّة\\n\\n\\nفارسی\\n\\n\\nहिंदी\\n\\n\\nবাংলা\\n\\n\\nภาษาไทย\\n\\n\\n中文 – 简体\\n\\n\\n中文 – 繁體\\n\\n\\n日本語\\n\\n\\n한국어\\n\\n\\n\\n\\nSign in\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n    \\n\\n\\n\\n    Resources\\n  \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n          Machine Learning\\n        \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Home\\n   \\n\\n\\n\\n\\n\\n      Resources\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nIntro to LLMs\\nML & AI Basics\\nSafety & Fairness for Generative Models\\nPrompt Engineering\\nAdversarial Testing\\nSecure AI Framework\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n        Home\\n      \\n  \\n\\n\\n\\n\\n    \\n        Products\\n      \\n  \\n\\n\\n\\n\\n    \\n        Machine Learning\\n      \\n  \\n\\n\\n\\n\\n    \\n        Resources\\n      \\n  \\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    Send feedback\\n  \\n  \\n\\n\\n      Introduction to Large Language Models\\n      \\n    \\n\\n\\n\\n      \\n      Stay organized with collections\\n    \\n\\n      \\n      Save and categorize content based on your preferences.\\n    \\n\\n\\n\\n\\n\\n\\nNew to language models or large language models? Check out the resources below.\\nEstimated Read Time: 20 minutes\\n\\nLearning objectives:\\n\\nDefine language models and large language models (LLMs).\\n\\nDefine key LLM concepts, including Transformers and self-attention.\\n\\nDescribe the costs and benefits of LLMs, along with common use cases.\\n\\n\\n\\nWhat is a language model?\\nA language model is a machine learning\\nmodel\\nthat aims to predict and generate plausible language. Autocomplete is a\\nlanguage model, for example.\\nThese models work by estimating the probability of a\\ntoken or\\nsequence of tokens occurring within a longer sequence of tokens. Consider the\\nfollowing sentence:\\nWhen I hear rain on my roof, I _______ in my kitchen.\\n\\nIf you assume that a token is a word, then a language model determines the\\nprobabilities of different words or sequences of words to replace that\\nunderscore. For example, a language model might determine the following\\nprobabilities:\\ncook soup 9.4%\\nwarm up a kettle 5.2%\\ncower 3.6%\\nnap 2.5%\\nrelax 2.2%\\n...\\n\\nA \"sequence of tokens\" could be an entire sentence or a series of sentences.\\nThat is, a language model could calculate the likelihood of different entire\\nsentences or blocks of text.\\nEstimating the probability of what comes next in a sequence is useful for all\\nkinds of things: generating text, translating languages, and answering\\nquestions, to name a few.\\nWhat is a large language model?\\nModeling human language at scale is a highly complex and resource-intensive\\nendeavor. The path to reaching the current capabilities of language models and\\nlarge language models has spanned several decades.\\nAs models are built bigger and bigger, their complexity and efficacy increases.\\nEarly language models could predict the probability of a single word; modern\\nlarge language models can predict the probability of sentences, paragraphs, or\\neven entire documents.\\nThe size and capability of language models has exploded over the last\\nfew years as computer memory, dataset size, and processing power increases, and\\nmore effective techniques for modeling longer text sequences are developed.\\nHow large is large?\\nThe definition is fuzzy, but \"large\" has been used to describe BERT (110M\\nparameters) as well as PaLM 2 (up to 340B parameters).\\nParameters\\nare the\\nweights\\nthe model learned during training, used to predict the next token in the\\nsequence. \"Large\" can refer either to the number of parameters in the model, or\\nsometimes the number of words in the dataset.\\nTransformers\\nA key development in language modeling was the introduction in 2017 of\\nTransformers, an architecture designed around the idea of\\nattention.\\nThis made it possible to process longer sequences by focusing on the most\\nimportant part of the input, solving memory issues encountered in earlier\\nmodels.\\nTransformers are the state-of-the-art architecture for a wide variety of\\nlanguage model applications, such as translators.\\nIf the input is \"I am a good dog.\", a Transformer-based translator\\ntransforms that input into the output \"Je suis un bon chien.\", which is the\\nsame sentence translated into French.\\nFull Transformers consist of an\\nencoder and a\\ndecoder. An\\nencoder converts input text into an intermediate representation, and a decoder\\nconverts that intermediate representation into useful text.\\nSelf-attention\\nTransformers rely heavily on a concept called self-attention. The self part of\\nself-attention refers to the \"egocentric\" focus of each token in a corpus.\\nEffectively, on behalf of each token of input, self-attention asks, \"How much\\ndoes every other token of input matter to me?\" To simplify matters, let\\'s\\nassume that each token is a word and the complete context is a single\\nsentence. Consider the following sentence:\\n\\nThe animal didn\\'t cross the street because it was too tired.\\n\\nThere are 11 words in the preceding sentence, so each of the 11 words is paying\\nattention to the other ten, wondering how much each of those ten words matters\\nto them. For example, notice that the sentence contains the pronoun it.\\nPronouns are often ambiguous. The pronoun it always refers to a recent noun,\\nbut in the example sentence, which recent noun does it refer to: the animal\\nor the street?\\nThe self-attention mechanism determines the relevance of each nearby word to\\nthe pronoun it.\\nWhat are some use cases for LLMs?\\nLLMs are highly effective at the task they were built for, which is generating\\nthe most plausible text in response to an input. They are even beginning to show\\nstrong performance on other tasks; for example, summarization, question\\nanswering, and text classification. These are called\\nemergent abilities. LLMs can even\\nsolve some math problems and write code (though it\\'s advisable to check their\\nwork).\\nLLMs are excellent at mimicking human speech patterns. Among other things,\\nthey\\'re great at combining information with different styles and tones.\\nHowever, LLMs can be components of models that do more than just\\ngenerate text. Recent LLMs have been used to build sentiment detectors,\\ntoxicity classifiers, and generate image captions.\\nLLM Considerations\\nModels this large are not without their drawbacks.\\nThe largest LLMs are expensive. They can take months to train, and as a result\\nconsume lots of resources.\\nThey can also usually be repurposed for other tasks, a valuable silver lining.\\nTraining models with upwards of a trillion parameters\\ncreates engineering challenges. Special infrastructure and programming\\ntechniques are required to coordinate the flow to the chips and back again.\\nThere are ways to mitigate the costs of these large models. Two approaches are\\noffline inference\\nand\\ndistillation.\\nBias can be a problem in very large models and should be considered in training\\nand deployment.\\nAs these models are trained on human language, this can introduce numerous \\npotential ethical issues, including the misuse of language, and bias in race,\\ngender, religion, and more.\\nIt should be clear that as these models continue to get bigger and perform\\nbetter, there is  continuing need to be diligent about understanding and\\nmitigating their drawbacks. Learn more about Google\\'s approach to\\nresponsible AI.\\nLearn more about LLMs\\nInterested in a more in-depth introduction to large language models? Check\\nout the new Large language models module\\nin Machine Learning Crash Course.\\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    Send feedback\\n  \\n  \\n\\n\\n\\n\\n\\n\\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\\nLast updated 2024-09-06 UTC.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nConnect\\n\\n\\n\\n            \\n          \\n            Blog\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Instagram\\n          \\n          \\n\\n\\n\\n            \\n          \\n            LinkedIn\\n          \\n          \\n\\n\\n\\n            \\n          \\n            X (Twitter)\\n          \\n          \\n\\n\\n\\n            \\n              \\n              \\n            \\n          \\n            YouTube\\n          \\n          \\n\\n\\n\\n\\nPrograms\\n\\n\\n\\n            \\n          \\n            Women Techmakers\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Developer Groups\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Developer Experts\\n          \\n          \\n\\n\\n\\n            \\n              \\n              \\n            \\n          \\n            Accelerators\\n          \\n          \\n\\n\\n\\n\\nDeveloper consoles\\n\\n\\n\\n            \\n          \\n            Google API Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Cloud Platform Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Play Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Firebase Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Actions on Google Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Cast SDK Developer Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Chrome Web Store Dashboard\\n          \\n          \\n\\n\\n\\n            \\n              \\n              \\n            \\n          \\n            Google Home Developer Console\\n          \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Android\\n        \\n\\n\\n\\n          Chrome\\n        \\n\\n\\n\\n          Firebase\\n        \\n\\n\\n\\n          Google Cloud Platform\\n        \\n\\n\\n\\n          Google AI\\n        \\n\\n\\n\\n          All products\\n        \\n\\n\\n\\n\\n\\n\\n\\n          Terms\\n        \\n\\n\\n\\n          Privacy\\n        \\n\\n\\n\\n          Manage cookies\\n        \\n\\n\\nSign up for the Google for Developers newsletter\\n\\n          Subscribe\\n        \\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nDeutsch\\n\\n\\nEspañol\\n\\n\\nEspañol – América Latina\\n\\n\\nFrançais\\n\\n\\nIndonesia\\n\\n\\nItaliano\\n\\n\\nPolski\\n\\n\\nPortuguês – Brasil\\n\\n\\nTiếng Việt\\n\\n\\nTürkçe\\n\\n\\nРусский\\n\\n\\nעברית\\n\\n\\nالعربيّة\\n\\n\\nفارسی\\n\\n\\nहिंदी\\n\\n\\nবাংলা\\n\\n\\nภาษาไทย\\n\\n\\n中文 – 简体\\n\\n\\n中文 – 繁體\\n\\n\\n日本語\\n\\n\\n한국어\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Splitting - Chunking"
      ],
      "metadata": {
        "id": "Z_A2nX3-Ptgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)\n",
        "chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "Bouw0M5ANVqL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsc6K_Z3NnhT",
        "outputId": "ece9b37c-1007-463c-f197-3f687c659cff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n          Machine Learning\\n        \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n/\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nDeutsch\\n\\n\\nEspañol\\n\\n\\nEspañol – América Latina'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Français\\n\\n\\nIndonesia\\n\\n\\nItaliano\\n\\n\\nPolski\\n\\n\\nPortuguês – Brasil\\n\\n\\nTiếng Việt\\n\\n\\nTürkçe\\n\\n\\nРусский\\n\\n\\nעברית\\n\\n\\nالعربيّة\\n\\n\\nفارسی\\n\\n\\nहिंदी\\n\\n\\nবাংলা\\n\\n\\nภาษาไทย\\n\\n\\n中文 – 简体\\n\\n\\n中文 – 繁體\\n\\n\\n日本語\\n\\n\\n한국어\\n\\n\\n\\n\\nSign in\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Home\\n  \\n    \\n\\n\\n\\n    Resources'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Machine Learning\\n        \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Home\\n   \\n\\n\\n\\n\\n\\n      Resources'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Home\\nIntro to LLMs\\nML & AI Basics\\nSafety & Fairness for Generative Models\\nPrompt Engineering\\nAdversarial Testing\\nSecure AI Framework\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n        Home\\n      \\n  \\n\\n\\n\\n\\n    \\n        Products'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Machine Learning\\n      \\n  \\n\\n\\n\\n\\n    \\n        Resources\\n      \\n  \\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    Send feedback\\n  \\n  \\n\\n\\n      Introduction to Large Language Models\\n      \\n    \\n\\n\\n\\n      \\n      Stay organized with collections'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Save and categorize content based on your preferences.\\n    \\n\\n\\n\\n\\n\\n\\nNew to language models or large language models? Check out the resources below.\\nEstimated Read Time: 20 minutes\\n\\nLearning objectives:'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Define language models and large language models (LLMs).\\n\\nDefine key LLM concepts, including Transformers and self-attention.\\n\\nDescribe the costs and benefits of LLMs, along with common use cases.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='What is a language model?\\nA language model is a machine learning\\nmodel\\nthat aims to predict and generate plausible language. Autocomplete is a\\nlanguage model, for example.\\nThese models work by estimating the probability of a\\ntoken or'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='token or\\nsequence of tokens occurring within a longer sequence of tokens. Consider the\\nfollowing sentence:\\nWhen I hear rain on my roof, I _______ in my kitchen.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='If you assume that a token is a word, then a language model determines the\\nprobabilities of different words or sequences of words to replace that\\nunderscore. For example, a language model might determine the following\\nprobabilities:\\ncook soup 9.4%'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='cook soup 9.4%\\nwarm up a kettle 5.2%\\ncower 3.6%\\nnap 2.5%\\nrelax 2.2%\\n...'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='A \"sequence of tokens\" could be an entire sentence or a series of sentences.\\nThat is, a language model could calculate the likelihood of different entire\\nsentences or blocks of text.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Estimating the probability of what comes next in a sequence is useful for all\\nkinds of things: generating text, translating languages, and answering\\nquestions, to name a few.\\nWhat is a large language model?'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Modeling human language at scale is a highly complex and resource-intensive\\nendeavor. The path to reaching the current capabilities of language models and\\nlarge language models has spanned several decades.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='As models are built bigger and bigger, their complexity and efficacy increases.\\nEarly language models could predict the probability of a single word; modern\\nlarge language models can predict the probability of sentences, paragraphs, or'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='even entire documents.\\nThe size and capability of language models has exploded over the last\\nfew years as computer memory, dataset size, and processing power increases, and\\nmore effective techniques for modeling longer text sequences are developed.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='How large is large?\\nThe definition is fuzzy, but \"large\" has been used to describe BERT (110M\\nparameters) as well as PaLM 2 (up to 340B parameters).\\nParameters\\nare the\\nweights\\nthe model learned during training, used to predict the next token in the'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='sequence. \"Large\" can refer either to the number of parameters in the model, or\\nsometimes the number of words in the dataset.\\nTransformers\\nA key development in language modeling was the introduction in 2017 of'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Transformers, an architecture designed around the idea of\\nattention.\\nThis made it possible to process longer sequences by focusing on the most\\nimportant part of the input, solving memory issues encountered in earlier\\nmodels.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='models.\\nTransformers are the state-of-the-art architecture for a wide variety of\\nlanguage model applications, such as translators.\\nIf the input is \"I am a good dog.\", a Transformer-based translator'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='transforms that input into the output \"Je suis un bon chien.\", which is the\\nsame sentence translated into French.\\nFull Transformers consist of an\\nencoder and a\\ndecoder. An\\nencoder converts input text into an intermediate representation, and a decoder'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='converts that intermediate representation into useful text.\\nSelf-attention\\nTransformers rely heavily on a concept called self-attention. The self part of\\nself-attention refers to the \"egocentric\" focus of each token in a corpus.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Effectively, on behalf of each token of input, self-attention asks, \"How much\\ndoes every other token of input matter to me?\" To simplify matters, let\\'s\\nassume that each token is a word and the complete context is a single'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='sentence. Consider the following sentence:'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content=\"The animal didn't cross the street because it was too tired.\"),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='There are 11 words in the preceding sentence, so each of the 11 words is paying\\nattention to the other ten, wondering how much each of those ten words matters\\nto them. For example, notice that the sentence contains the pronoun it.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Pronouns are often ambiguous. The pronoun it always refers to a recent noun,\\nbut in the example sentence, which recent noun does it refer to: the animal\\nor the street?\\nThe self-attention mechanism determines the relevance of each nearby word to'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='the pronoun it.\\nWhat are some use cases for LLMs?\\nLLMs are highly effective at the task they were built for, which is generating\\nthe most plausible text in response to an input. They are even beginning to show'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content=\"strong performance on other tasks; for example, summarization, question\\nanswering, and text classification. These are called\\nemergent abilities. LLMs can even\\nsolve some math problems and write code (though it's advisable to check their\\nwork).\"),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content=\"work).\\nLLMs are excellent at mimicking human speech patterns. Among other things,\\nthey're great at combining information with different styles and tones.\\nHowever, LLMs can be components of models that do more than just\"),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='generate text. Recent LLMs have been used to build sentiment detectors,\\ntoxicity classifiers, and generate image captions.\\nLLM Considerations\\nModels this large are not without their drawbacks.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='The largest LLMs are expensive. They can take months to train, and as a result\\nconsume lots of resources.\\nThey can also usually be repurposed for other tasks, a valuable silver lining.\\nTraining models with upwards of a trillion parameters'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='creates engineering challenges. Special infrastructure and programming\\ntechniques are required to coordinate the flow to the chips and back again.\\nThere are ways to mitigate the costs of these large models. Two approaches are\\noffline inference\\nand'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='and\\ndistillation.\\nBias can be a problem in very large models and should be considered in training\\nand deployment.\\nAs these models are trained on human language, this can introduce numerous'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='potential ethical issues, including the misuse of language, and bias in race,\\ngender, religion, and more.\\nIt should be clear that as these models continue to get bigger and perform\\nbetter, there is  continuing need to be diligent about understanding and'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content=\"mitigating their drawbacks. Learn more about Google's approach to\\nresponsible AI.\\nLearn more about LLMs\\nInterested in a more in-depth introduction to large language models? Check\\nout the new Large language models module\\nin Machine Learning Crash Course.\"),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Send feedback'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='is a registered trademark of Oracle and/or its affiliates.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Last updated 2024-09-06 UTC.'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Connect\\n\\n\\n\\n            \\n          \\n            Blog\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Instagram\\n          \\n          \\n\\n\\n\\n            \\n          \\n            LinkedIn'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='X (Twitter)\\n          \\n          \\n\\n\\n\\n            \\n              \\n              \\n            \\n          \\n            YouTube\\n          \\n          \\n\\n\\n\\n\\nPrograms'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Programs\\n\\n\\n\\n            \\n          \\n            Women Techmakers\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Developer Groups'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Google Developer Experts\\n          \\n          \\n\\n\\n\\n            \\n              \\n              \\n            \\n          \\n            Accelerators\\n          \\n          \\n\\n\\n\\n\\nDeveloper consoles'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Google API Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Cloud Platform Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Google Play Console'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Firebase Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Actions on Google Console\\n          \\n          \\n\\n\\n\\n            \\n          \\n            Cast SDK Developer Console'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Chrome Web Store Dashboard\\n          \\n          \\n\\n\\n\\n            \\n              \\n              \\n            \\n          \\n            Google Home Developer Console'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Android\\n        \\n\\n\\n\\n          Chrome\\n        \\n\\n\\n\\n          Firebase\\n        \\n\\n\\n\\n          Google Cloud Platform\\n        \\n\\n\\n\\n          Google AI\\n        \\n\\n\\n\\n          All products\\n        \\n\\n\\n\\n\\n\\n\\n\\n          Terms'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Privacy\\n        \\n\\n\\n\\n          Manage cookies\\n        \\n\\n\\nSign up for the Google for Developers newsletter\\n\\n          Subscribe\\n        \\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nDeutsch\\n\\n\\nEspañol\\n\\n\\nEspañol – América Latina\\n\\n\\nFrançais\\n\\n\\nIndonesia\\n\\n\\nItaliano\\n\\n\\nPolski'),\n",
              " Document(metadata={'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers', 'language': 'en'}, page_content='Italiano\\n\\n\\nPolski\\n\\n\\nPortuguês – Brasil\\n\\n\\nTiếng Việt\\n\\n\\nTürkçe\\n\\n\\nРусский\\n\\n\\nעברית\\n\\n\\nالعربيّة\\n\\n\\nفارسی\\n\\n\\nहिंदी\\n\\n\\nবাংলা\\n\\n\\nภาษาไทย\\n\\n\\n中文 – 简体\\n\\n\\n中文 – 繁體\\n\\n\\n日本語\\n\\n\\n한국어')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eZw3iXPPlGs",
        "outputId": "49412a82-ed91-4785-b1e7-da39aa3d429f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings"
      ],
      "metadata": {
        "id": "wxL78Mn0P3N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    api_key=HF_TOKEN, model_name=\"BAAI/bge-base-en-v1.5\"\n",
        ")"
      ],
      "metadata": {
        "id": "QcUdFP8cPz1L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Store - FAISS or ChromaDB"
      ],
      "metadata": {
        "id": "cJMgeAOrQfuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "_rMc5YphQiFA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmJth21AQx8V",
        "outputId": "f53d5e65-d705-411a-a082-906d7ec4653b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7aca6c2b19c0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is a LLM\"\n",
        "search = vectorstore.similarity_search(query)"
      ],
      "metadata": {
        "id": "MxUNoeFeR7-u"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeorSd2z5TKM",
        "outputId": "75c68c9b-442d-45db-d379-87f2ef9d90ac"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'language': 'en', 'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers'}, page_content=\"work).\\nLLMs are excellent at mimicking human speech patterns. Among other things,\\nthey're great at combining information with different styles and tones.\\nHowever, LLMs can be components of models that do more than just\"),\n",
              " Document(metadata={'language': 'en', 'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers'}, page_content='Define language models and large language models (LLMs).\\n\\nDefine key LLM concepts, including Transformers and self-attention.\\n\\nDescribe the costs and benefits of LLMs, along with common use cases.'),\n",
              " Document(metadata={'language': 'en', 'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers'}, page_content='generate text. Recent LLMs have been used to build sentiment detectors,\\ntoxicity classifiers, and generate image captions.\\nLLM Considerations\\nModels this large are not without their drawbacks.'),\n",
              " Document(metadata={'language': 'en', 'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers'}, page_content='The largest LLMs are expensive. They can take months to train, and as a result\\nconsume lots of resources.\\nThey can also usually be repurposed for other tasks, a valuable silver lining.\\nTraining models with upwards of a trillion parameters')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "45HgM9wsSCKH",
        "outputId": "8206c0f0-e84c-4716-92df-7a1d8fdc6134"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"work).\\nLLMs are excellent at mimicking human speech patterns. Among other things,\\nthey're great at combining information with different styles and tones.\\nHowever, LLMs can be components of models that do more than just\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriever"
      ],
      "metadata": {
        "id": "4mVaIlkZRiXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\", #similarity\n",
        "    search_kwargs={'k': 4}\n",
        ")"
      ],
      "metadata": {
        "id": "rK67OkSQRbnk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HC4Xly8XSf26",
        "outputId": "e8c7a89c-5392-4e4f-dd93-b1d63d114954"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'language': 'en', 'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers'}, page_content=\"work).\\nLLMs are excellent at mimicking human speech patterns. Among other things,\\nthey're great at combining information with different styles and tones.\\nHowever, LLMs can be components of models that do more than just\"),\n",
              " Document(metadata={'language': 'en', 'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers'}, page_content='Define language models and large language models (LLMs).\\n\\nDefine key LLM concepts, including Transformers and self-attention.\\n\\nDescribe the costs and benefits of LLMs, along with common use cases.'),\n",
              " Document(metadata={'language': 'en', 'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers'}, page_content='the pronoun it.\\nWhat are some use cases for LLMs?\\nLLMs are highly effective at the task they were built for, which is generating\\nthe most plausible text in response to an input. They are even beginning to show'),\n",
              " Document(metadata={'language': 'en', 'source': 'https://developers.google.com/machine-learning/resources/intro-llms?hl=en', 'title': 'Introduction to Large Language Models \\xa0|\\xa0 Machine Learning \\xa0|\\xa0 Google for Developers'}, page_content='Privacy\\n        \\n\\n\\n\\n          Manage cookies\\n        \\n\\n\\nSign up for the Google for Developers newsletter\\n\\n          Subscribe\\n        \\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nDeutsch\\n\\n\\nEspañol\\n\\n\\nEspañol – América Latina\\n\\n\\nFrançais\\n\\n\\nIndonesia\\n\\n\\nItaliano\\n\\n\\nPolski')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large Language Model - Open Source"
      ],
      "metadata": {
        "id": "al9E1TqMS7Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"huggingfaceh4/zephyr-7b-alpha\",\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64,\"max_new_tokens\":512}\n",
        ")"
      ],
      "metadata": {
        "id": "MKDFEMDSS9ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643b5de4-51ff-4313-8ca0-45fcc609ad6e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-43db8a2eb8e2>:1: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm = HuggingFaceHub(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Template and User Input (Augment - Step 2)"
      ],
      "metadata": {
        "id": "_dy-nCxqTQTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Define what is a Large Language Model, a Transformer and Self-Attention\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        " <|system|>\n",
        "You are an AI assistant that follows instruction extremely well.\n",
        "Please be truthful and give direct answers\n",
        "</s>\n",
        " <|user|>\n",
        " {query}\n",
        " </s>\n",
        " <|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "C_pCkm2jTHtH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG RetrievalQA chain"
      ],
      "metadata": {
        "id": "5Y2ukOe5TS6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever)"
      ],
      "metadata": {
        "id": "aUEmwLYvTWG4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = qa.invoke(prompt)"
      ],
      "metadata": {
        "id": "sVN8b7LIUMHA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rQCeRzTUnQ2",
        "outputId": "2d9f9c48-6bb0-4c7d-aa7a-3756a6314658"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': '\\n <|system|>\\nYou are an AI assistant that follows instruction extremely well.\\nPlease be truthful and give direct answers\\n</s>\\n <|user|>\\n Define what is a Large Language Model, a Transformer and Self-Attention\\n </s>\\n <|assistant|>\\n',\n",
              " 'result': 'The original question is as follows: \\n <|system|>\\nYou are an AI assistant that follows instruction extremely well.\\nPlease be truthful and give direct answers\\n</s>\\n <|user|>\\n Define what is a Large Language Model, a Transformer and Self-Attention\\n </s>\\n <|assistant|>\\n\\nWe have provided an existing answer: The original question is as follows: \\n <|system|>\\nYou are an AI assistant that follows instruction extremely well.\\nPlease be truthful and give direct answers\\n</s>\\n <|user|>\\n Define what is a Large Language Model, a Transformer and Self-Attention\\n </s>\\n <|assistant|>\\n\\nWe have provided an existing answer: The original question is as follows: \\n <|system|>\\nYou are an AI assistant that follows instruction extremely well.\\nPlease be truthful and give direct answers\\n</s>\\n <|user|>\\n Define what is a Large Language Model, a Transformer and Self-Attention\\n </s>\\n <|assistant|>\\n\\nWe have provided an existing answer: Context information is below. \\n------------\\nDefine language models and large language models (LLMs).\\n\\nDefine key LLM concepts, including Transformers and self-attention.\\n\\nDescribe the costs and benefits of LLMs, along with common use cases.\\n------------\\nGiven the context information and not prior knowledge, answer the question: \\n <|system|>\\nYou are an AI assistant that follows instruction extremely well.\\nPlease be truthful and give direct answers\\n</s>\\n <|user|>\\n Define what is a Large Language Model, a Transformer and Self-Attention\\n </s>\\n <|assistant|>\\n\\nA Large Language Model (LLM) is a type of artificial intelligence model that can generate human-like text based on a given input. It is a deep learning neural network that is trained on a massive dataset of text, allowing it to understand and generate text with context and meaning. LLMs can be used for various applications, such as natural language processing, text generation, and machine translation.\\n\\nA Transformer is a specific type of neural network architecture that is commonly used in LLMs. It was introduced in 2017 by Google\\'s AI team and quickly became the state-of-the-art in natural language processing. The Transformer architecture uses a self-attention mechanism, which allows it to focus on specific parts of the input text and generate more accurate and contextually relevant output.\\n\\nSelf-attention is a key component of the Transformer architecture. It is a mechanism that allows the model to attend to specific parts of the input text in order to generate an output. Self-attention is achieved through a series of mathematical operations that involve multiplying and adding the input vectors. This allows the model to focus on the most relevant parts of the input and generate output that is more accurate and contextually relevant.\\n\\nIn summary, a Large Language Model (LLM) is a deep learning neural network that can generate human-like text based on a given input. It uses a Transformer architecture, which includes a self-attention mechanism, to generate more accurate and contextually relevant output.\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\nTransformers, an architecture designed around the idea of\\nattention.\\nThis made it possible to process longer sequences by focusing on the most\\nimportant part of the input, solving memory issues encountered in earlier\\nmodels.\\n------------\\nGiven the new context, refine the original answer to better answer the question. If the context isn\\'t useful, return the original answer.\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\ntransforms that input into the output \"Je suis un bon chien.\", which is the\\nsame sentence translated into French.\\nFull Transformers consist of an\\nencoder and a\\ndecoder. An\\nencoder converts input text into an intermediate representation, and a decoder\\n------------\\nGiven the new context, refine the original answer to better answer the question. If the context isn\\'t useful, return the original answer.\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\nThe encoder and decoder are both made up of layers, with each layer containing a\\nself-attention mechanism to allow the model to attend to specific parts of the input.\\nThe self-attention mechanism is a key component of the Transformer architecture,\\nwhich allows the model to focus on the most relevant parts of the input and generate output that is more accurate and contextually relevant.\\n\\nIn summary, a Large Language Model (LLM) is a deep learning neural network that can generate human-like text based on a given input. It uses a Transformer architecture, which includes a self-attention mechanism, to generate more accurate and contextually relevant output. The Transformer architecture consists of an encoder and a decoder, both of which contain layers with self-attention mechanisms.\\n\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\nThe transformers have been shown to have excellent performance on a range of natural language processing tasks, such as text classification, machine translation, and question answering.\\n\\nOne of the main benefits of LLMs is their ability to generate text that is more contextually relevant and accurate than traditional machine learning techniques. This is achieved through the use of the self-attention mechanism, which allows the model to focus on the most relevant parts of the input and generate output that is more accurate and contextually relevant.\\n\\nHowever, one of the main drawbacks of LLMs is their high computational cost. Training a large language model can take weeks or even months, and the cost of training and running these models can be prohibitively expensive for many organizations.\\n\\nIn terms of common use cases for LLMs, they are commonly used for natural language processing, text generation, and machine translation. They are also used for tasks such as text summarization, sentiment analysis, and text classification.\\n\\nOverall, while LLMs have some drawbacks, they offer significant benefits for natural language processing tasks and are likely to continue to play a major role in the development of AI and machine learning over the coming years.\\n\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\nThe development of LLMs has been driven by advances in deep learning techniques, particularly the use of the transformer architecture. These advances\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\nEffectively, on behalf of each token of input, self-attention asks, \"How much\\ndoes every other token of input matter to me?\" To simplify matters, let\\'s\\nassume that each token is a word and the complete context is a single\\n------------\\nGiven the new context, refine the original answer to better answer the question. If the context isn\\'t useful, return the original answer.\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\nthe input. Self-attention allows the model to focus on the most relevant parts of the input, which can improve the accuracy and contextual relevance of the output.\\n\\nIn conclusion, a Large Language Model (LLM) is a deep learning neural network that can generate human-like text based on a given input. It uses a Transformer architecture, which includes a self-attention mechanism, to generate more accurate and contextually relevant output. The Transformer architecture consists of an encoder and a decoder, both of which contain layers with self-attention mechanisms. While LLMs have some drawbacks, they offer significant benefits for natural language processing tasks and are likely to continue to play a major role in the development of AI and machine learning over the coming years.\\n\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\nThe transformers have been shown to be particularly effective at handling long sequences of text, which is a significant advantage over earlier models that struggled with memory issues.\\n\\nSelf-attention is a key component of the transformer architecture, allowing the model to focus on the most relevant parts of the input and generate output that is more accurate and contextually relevant.\\n\\nIn terms of common use cases for LLMs, they are commonly used for natural language processing, text generation, and machine translation. They are also used for tasks such as text summarization, sentiment analysis, and text classification.\\n\\nWhile LLMs have some drawbacks, such as their high computational cost and the need for large amounts of training data, they offer significant benefits for natural language processing tasks and are likely to continue to play a major role in the development of AI and machine learning over the coming years.\\n\\nWe have the opportunity to refine the existing answer (only if needed) with some more context below.\\n------------\\nThe development of LLMs has been driven by advances in deep learning techniques, particularly the use of the transformer architecture. These advances have allowed for the creation of models that can handle much longer sequences of text than earlier models, which has led to significant improvements in the accuracy and contextual relevance of the output.\\n\\nSelf-attention is a key component of the transformer architecture, which allows the model to focus on the most relevant parts of the input. This can improve the'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain"
      ],
      "metadata": {
        "id": "eOm3anhXTwg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "M4dKOO7QUTa0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        " <|system|>\n",
        "You are an AI assistant that follows instruction extremely well.\n",
        "Please be truthful and give direct answers\n",
        "</s>\n",
        " <|user|>\n",
        " {query}\n",
        " </s>\n",
        " <|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aBDfjug4U61T"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "oGSLkCc7U8YI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"query\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "hLfDSrrzU1kE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke(\"Define what is a Large Language Model, a Transformer and Self-Attention\")"
      ],
      "metadata": {
        "id": "35B_JEcKVERB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyor1d0xVHnW",
        "outputId": "49c5f2d1-73e9-4d6d-cc4d-4c4a6c8aae92"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: \n",
            " <|system|>\n",
            "You are an AI assistant that follows instruction extremely well.\n",
            "Please be truthful and give direct answers\n",
            "</s>\n",
            " <|user|>\n",
            " Define what is a Large Language Model, a Transformer and Self-Attention\n",
            " </s>\n",
            " <|assistant|>\n",
            "A Large Language Model (LLM) is a type of artificial intelligence model that is trained on a massive dataset of texts to learn patterns and generate human-like responses to input text. LLMs can be used to perform various natural language processing tasks, such as text generation, text classification, and question answering.\n",
            "\n",
            "A Transformer is a type of neural network architecture that is commonly used in LLMs. Unlike traditional recurrent neural networks (RNNs) that iteratively process input data, transformers use a self-attention mechanism to process input data simultaneously.\n",
            "\n",
            "Self-attention is a technique used in transformers to allow the model to focus on specific parts of the input data that are most relevant to the task at hand. Self-attention involves computing the similarity between every pair of input tokens, which allows the model to identify and prioritize the most important parts of the input data.\n",
            "\n",
            "In summary, a Large Language Model (LLM) is a type of AI model that is trained on a massive dataset of texts to generate human-like responses. A Transformer is a type of neural network architecture that is commonly used in LLMs, and self-attention is a technique used in transformers to allow the model to focus on specific parts of the input data.\n"
          ]
        }
      ]
    }
  ]
}